\section{Introduction}

In conditionally randomized experiments, such as clinical trials, researchers often have to specify the probability of assigning treatments based on measured features/covariates. In order for researchers to determine how to assign treatments--which following the contextual bandit literature, we will denote as a \textit{policy}--they need to understand how well the policy will perform in terms of a key outcome of interest, such as patient outcomes in a clinical trial. Note that in bandit/reinforcement contexts, this outcome is called a \textit{reward}. Questions of interest include how the policy will perform on average, how the policy will affected certain "protected" or vulnerable groups (such as children), and who will benefit most and who will suffer most under a given policy. A policy can be stationary, where stationary denotes that the policy is constant over time; or a policy can be nonstationary, meaning that it updates its assignment mechanism based on the observed history. Our paper considers both types of policies. \\

 \cite{dudik2014doubly} propose a doubly robust estimator for offline policy evaluation that given a history of observations under an old policy, estimates the average expected reward under a new "target" policy. The average expected reward under a policy is known as the value function. Their estimator is unbiased for the value function even if one of the two nuisance functions is biased (or misspecified)--hence the "double robustness." Doubly robust estimators are often regarded as the optimal estimators in causal inference because of their robustness to bias and because their rate of convergence depends on a product of errors of the nuisance terms. Thus, they can achieve fast rates of convergence under milder requirements on the convergence rates of the nuisance functions. \\

Despite the advantages of the doubly robust estimator, there are limitations with \cite{dudik2014doubly}'s method of offline policy evaluation. Coarse summaries like an average obfuscate many details about the policy's performance. They do not tell us where/why the policy is performing well (or poorly). They cannot help us analyze how fair or equitable a given policy is. Consider as a trivial example comparing two policies that have the same average outcome $\Psi$ where under policy $A$ all patients are treated with a "good" treatment and all patients have outcome $\Psi$ versus under policy $B$, half the patients are assigned an "excellent" treatment and therefore have outcome $2\Psi$ but the other half are assigned no treatment and have outcome = 0. While the average may be the same, the fairness of the two policies are drastically different. \\

Recent advances in causal inference have moved beyond estimating the average treatment effect (ATE) to estimating conditional or heterogeneous treatment effects (HTE), which considers the treatment effect for a particular subgroup as described by a set of covariates. These methods tell us the covariates (equivalently, contexts in the bandit setting) where the treatment works well and where the treatment does not work or even performs poorly. Applying insights from these methods can help us better understand our target policy's predicted performance. Recent papers focus on estimating effects of continuous random variables in the binary treatment setting. \cite{wager2017estimation} and \cite{athey2016recursive} use decision trees and random forests to estimate HTE's. They prove that their estimator is consistent and asymptotically normal under assumptions that may be hard to meet in practice, such as scaling the subsampling rate appropriately with the sample size. \cite{imai2013estimating} use support vector machines to estimate treatment effect heterogeneity in a randomized binary treatment trial. They require parametric assumptions and do not analyze the statistical properties of their estimator. \\

In this paper we propose and analyze a doubly robust estimator for subgroup policy evaluation in an offline setting, which we term $\mathcal{S}$-specific value functions. Our estimator is unbiased as long as one of two nuisance functions is estimated consistently. We show that our estimator can achieve $\sqrt{n}$-consistency and asymptotic normality under assumptions of positivity, consistency, exchangeability, sample-splitting, smoothness or sparsity in the nuisance functions, and a requirement that the estimates of the old policy be bounded away from zero. In certain applications there may be clear subgroups that are a priori of interest, such as protected groups or minorities; whereas in other settings we may be interested in empirically discovering which subgroups are most or least affected by the policy. For the latter case, we propose a tree-based method for finding subgroups of interest based on our doubly robust estimates of the $\mathcal{S}$-specific value functions.  \\

The paper is organized as follows: In section \ref{bandits} we review the literature on doubly robust offline policy evaluation. In section \ref{drest} we propose and analyze our $\mathcal{S}$-specific doubly robust estimator in the stationary setting. In section \ref{finding}, we propose a tree-based method for finding subgroups that benefit most and that suffer most under a new policy. Finally, in section \ref{nonstationary}, we show how our doubly robust $\mathcal{S}$-specific estimator can be used to evaluate nonstationary policies by modifying Algorithm 1 in \cite{dudik2014doubly}. \\

Offline policy evaluation is important in applications ranging from clinical trials and personalized medicine to social policy-making and online advertising. Offline heterogeneous policy evaluation enables us to gain deeper insight into the policy's performance and to identify the subgroups most at risk under a policy, which is essential in assessing a policy's fairness.

% Let's save these definitions for the section below so that we can define things like action, etc properly? But feel free to add to the medical example above to make the connection clear. "the patientâ€™s current characteristics are contextual infor- mation, a treatment is an action, and a DTR is a policy. Similar to contextual bandits, the quantity of interest in DTR can be expressed by a numeric reward signal related to the clinical outcome of a treatment." 
%Hence, we would like to apply heterogeneous treatment effect estimation regime to contextual bandits. Such a heterogeneous policy evaluation is important because...


%$
%&= \frac{1}{\sum_{k=1}^N \mathbb{I}{\{x_k \in \mathcal{S} \}}}  \sum_{k=1}^{N } \mathbb{I}{\{x_k \in \mathcal{S} \}} \phi(\pi, x_k)  - \mathbf{P}_{X_\mathcal{S}} \left[ \phi(\pi, x_k) \right] \label{eq1}\\
%&+ \mathbf{P}_{X_\mathcal{S}} \left[ \widehat{\phi}(\pi, x_k) \right] - \mathbf{P}_{X_\mathcal{S}} \left[ \phi(\pi, x_k) \right]  \label{eq2}\\
%&+ \left( \frac{1}{\sum_{k=1}^N \mathbb{I}{\{x_k \in \mathcal{S} \}}}  \sum_{k=1}^{N} \mathbb{I}{\{x_k \in \mathcal{S} \}} \left[ \widehat{\phi} (\pi, x_k) - \phi(\pi, x_k) \right] - \mathbf{P}_{X_\mathcal{S}} \left[ \widehat{\phi}(\pi, x_k) - \phi(\pi, x_k)  \right] \right). \label{eq3}$

 